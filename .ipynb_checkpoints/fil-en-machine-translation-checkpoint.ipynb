{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "import re\n",
    "import numpy as np\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "def load_text(file):\n",
    "    with open(file, mode='rt', encoding='utf-8') as f:\n",
    "        text = f.read().split('\\n')\n",
    "        return np.array([pair.split('\\t') for pair in text])\n",
    "######################################################################################################\n",
    "def clean_text(text):\n",
    "    text = np.char.lower(text)\n",
    "    text = np.vectorize(re.sub)('[^a-z ]', '', text)\n",
    "    return text\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['who', 'sino'],\n",
       "       ['hello', 'kamusta'],\n",
       "       ['i try', 'sinusubukan ko'],\n",
       "       ...,\n",
       "       ['no matter what your profession or how happy you may be in it there are moments when you wish you had chosen some other career',\n",
       "        'kahit anong trabaho mo o kahit gaano ka kasaya dito may mga pagkakataon na naiisip mo na sana ibang trabaho na lang ang pinili mo'],\n",
       "       ['she has a boyfriend shes been going out with since high school but she feels their relationship has stagnated so shes become dissatisfied',\n",
       "        'high school pa lamang ay sila na ng kasintahan niya ngunit nararamdaman niya na wala nang nangyayari sa pagsasama nila kaya hindi na siya masaya'],\n",
       "       ['no matter how much you try to convince people that chocolate is vanilla itll still be chocolate even though you may manage to convince yourself and a few others that its vanilla',\n",
       "        'kahit ano pang gawin mo para kumbinsihin ang mga tao na ang tsokolate ay vanilla tsokolate pa rin ito kahit pa makumbinsi mo ang sarili mo at ang ilang tao na vanilla ito']],\n",
       "      dtype='<U177')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = load_text('fil_en.txt')\n",
    "clean_text = clean_text(text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split, tokenize, encode, pad, and perform one-hot encoding \n",
    "############################################################################################\n",
    "# split dataset to training set and test set\n",
    "def split_train_test(text, n_text, n_train):\n",
    "    dataset = text[:n_text,:]\n",
    "    shuffle(dataset)\n",
    "    return dataset, dataset[:n_train], dataset[n_train:]\n",
    "############################################################################################\n",
    "# get maximum sentence length\n",
    "def max_length(text):\n",
    "    return max([len(line.split()) for line in text])\n",
    "############################################################################################\n",
    "# perform tokenization which maps each word/token to a number/index as a dictionary\n",
    "def tokenize(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "############################################################################################\n",
    "# encode input and output to integers \n",
    "# pad sequences to maximum text length, ensures each text has equal length\n",
    "def encode_text(tokenizer, max_length, text):\n",
    "    vector = tokenizer.texts_to_sequences(text)\n",
    "    return pad_sequences(vector, maxlen=max_length, padding='post') #list/array -->  array\n",
    "############################################################################################\n",
    "# one hot encode target sequence\n",
    "# transform each token into a vector with binary values and length equal to number of vocabulary\n",
    "def one_hot_encode(text, vocab_size):\n",
    "    return to_categorical(text, num_classes=vocab_size) \n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
